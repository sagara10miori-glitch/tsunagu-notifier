# ============================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================

import argparse
import os
import re
import time
import datetime
import requests

from utils.fetch import parse_html, validate_image_url
from utils.storage import load_json, save_json, append_json_list, clear_json
from utils.hashgen import generate_item_hash
from utils.shorturl import get_short_url
from utils.discord import send_discord


# ============================
# å¼•æ•°
# ============================

def parse_args():
    parser = argparse.ArgumentParser()

    # ãƒ­ã‚°åˆ¶å¾¡
    parser.add_argument("--quiet", action="store_true")
    parser.add_argument("--debug", action="store_true")

    # æ™‚é–“å¸¯å¼·åˆ¶
    parser.add_argument("--force-night", action="store_true")
    parser.add_argument("--force-day", action="store_true")

    # é€šçŸ¥åˆ¶å¾¡
    parser.add_argument("--dry-run", action="store_true")

    # Cloudflare å¯¾ç­–
    parser.add_argument("--retry", type=int, default=1)

    # seller_cache ã‚’ç„¡è¦–ã—ã¦å†å–å¾—
    parser.add_argument("--no-cache", action="store_true")

    return parser.parse_args()


# ============================
# è¨­å®š
# ============================

WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

URL_EXIST = (
    "https://tsunagu.cloud/exist_products"
    "?sort=&exist_product_category_id=2"
    "&exist_product_category2_id=2"
    "&exist_product_category3_id="
    "&keyword=&max_sales_count_exist_items=1"
    "&is_selling=true&is_ai_content=0"
)

URL_AUCTION = (
    "https://tsunagu.cloud/auctions"
    "?sort=&exist_product_category_id=2"
    "&exist_product_category2_id=2"
    "&exist_product_category3_id="
    "&keyword=&is_disp_progress=1&is_ai_content=0"
)

DATA_LAST = "data/last_all.json"
DATA_SELLER = "data/seller_cache.json"
DATA_PENDING_EXIST = "data/pending_night_exist.json"
DATA_PENDING_AUCTION = "data/pending_night_auction.json"

MAX_LAST = 5000
THIRTY_DAYS = 60 * 60 * 24 * 30


# ============================
# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿
# ============================

def load_exclude_users(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {line.strip() for line in f if line.strip() and not line.startswith("#")}
    except FileNotFoundError:
        return set()


EXCLUDE_USERS = load_exclude_users("config/exclude_users.txt")


def load_special_users(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {line.strip() for line in f if line.strip() and not line.startswith("#")}
    except FileNotFoundError:
        return set()


SPECIAL_USERS = load_special_users("config/special_users.txt")


# ============================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================

def now():
    return datetime.datetime.utcnow() + datetime.timedelta(hours=9)


def is_night():
    h = now().hour
    return 2 <= h < 6


def is_night_forced(args):
    if args.force_night:
        return True
    if args.force_day:
        return False
    return is_night()


def is_morning():
    t = now()
    return t.hour == 6 and t.minute == 0


def normalize_price(s):
    digits = "".join(c for c in s if c.isdigit())
    return f"{int(digits):,}å††" if digits else "0å††"


# URL æ­£è¦åŒ–ï¼ˆå•†å“IDéƒ¨åˆ†ã ã‘ã‚’æŠ½å‡ºãƒ»æºã‚Œå¸åï¼‰
_URL_RE = re.compile(r"(?:https?:)?//?[^/]*?(auctions|exist_products)/(\d+)")

def normalize_url(url):
    m = _URL_RE.search(url)
    if m:
        category = m.group(1)
        item_id = m.group(2)
        return f"{category}/{item_id}"
    return url.strip().rstrip("/")


# ============================
# Cloudflare ã«å¼·ã„ HTML fetch
# ============================

def fetch_html(url, retry=1):
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/121.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    }

    proxy = os.getenv("PROXY_URL")
    proxies = {"http": proxy, "https": proxy} if proxy else None

    for t in range(retry):
        try:
            r = requests.get(url, headers=headers, proxies=proxies, timeout=6)
            r.raise_for_status()
            return r.text
        except Exception:
            time.sleep(1.2 * (t + 1))

    return ""


# ============================
# seller_id æŠ½å‡º
# ============================

seller_cache = {}

def fetch_seller_id(url, no_cache=False):
    if not no_cache and url in seller_cache:
        return seller_cache[url]

    html = fetch_html(url)
    if not html:
        seller_cache[url] = ""
        return ""

    soup = parse_html(html)
    if not soup:
        seller_cache[url] = ""
        return ""

    for pat in ["/users/", "/profile/"]:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if pat in href:
                m = re.search(pat + r"([^/?#]+)", href)
                if m:
                    seller_cache[url] = m.group(1)
                    return seller_cache[url]

    seller_cache[url] = ""
    return ""


# ============================
# HTML è§£æ
# ============================

def parse_items(soup, mode):
    items = []

    for c in soup.find_all(class_="p-product"):
        t = c.find(class_="title")
        title = t.get_text(strip=True) if t else ""

        price_tag = c.find("p", class_=lambda x: x and "text-danger" in x)
        if not price_tag:
            for tag in c.find_all(["p", "h2", "h3"]):
                txt = tag.get_text(strip=True)
                if ("å††" in txt or "Â¥" in txt) and any(ch.isdigit() for ch in txt):
                    price_tag = tag
                    break

        price = normalize_price(price_tag.get_text(strip=True) if price_tag else "")

        buy_now = None
        h2 = c.find("h2")
        if h2 and ("å³æ±º" in h2.text):
            buy_now = normalize_price(h2.text)

        url = c.find("a")["href"]
        if url.startswith("//"):
            url = "https:" + url
        elif url.startswith("/"):
            url = "https://tsunagu.cloud" + url

        img_tag = c.find("img")
        thumb = img_tag["src"] if img_tag else ""

        items.append(
            {
                "title": title,
                "price": price,
                "buy_now": buy_now,
                "thumb": thumb,
                "url": url,
                "mode": mode,
            }
        )

    return items


# ============================
# embed ç”Ÿæˆï¼ˆå„ªå…ˆåº¦ï¼‹è‰²ï¼‰
# ============================

def build_embed(item, seller):
    p = int(item["price"].replace("å††", "").replace(",", ""))

    if seller in SPECIAL_USERS:
        priority_icon = "ğŸ’Œ"
        priority_label = "å„ªå…ˆ"
        color = 0xFF66AA
    else:
        if p <= 3000:
            priority_icon = "ğŸ”¥"
            priority_label = "ç‰¹é¸"
            color = 0xFF4444
        elif p <= 5000:
            priority_icon = "â­"
            priority_label = "æ³¨ç›®"
            color = 0xFFDD33
        elif p <= 10000:
            priority_icon = "âœ¨"
            priority_label = "ãŠã™ã™ã‚"
            color = 0xF28C28
        else:
            priority_icon = ""
            priority_label = "é€šå¸¸"
            color = 0x66CCFF

    short = get_short_url(item["url"])

    fields = [
        {
            "name": "å„ªå…ˆåº¦",
            "value": f"{priority_icon} {priority_label}".strip(),
            "inline": True,
        },
        {
            "name": "è²©å£²å½¢å¼",
            "value": "æ—¢å­˜è²©å£²" if item["mode"] == "exist" else "ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³",
            "inline": True,
        },
        {
            "name": "ä¾¡æ ¼",
            "value": item["price"],
            "inline": True,
        },
    ]

    if item["buy_now"]:
        fields.append(
            {
                "name": "å³æ±ºä¾¡æ ¼",
                "value": item["buy_now"],
                "inline": True,
            }
        )

    embed = {
        "title": item["title"][:256],
        "url": short,
        "color": color,
        "fields": fields,
        "seller": seller,
    }

    img = validate_image_url(item["thumb"])
    if img:
        embed["image"] = {"url": img}

    return embed


# ============================
# å„ªå…ˆåº¦ã‚½ãƒ¼ãƒˆ
# ============================

def embed_priority(e):
    seller = e.get("seller", "")
    if seller in SPECIAL_USERS:
        pri = 0
    else:
        v = e["fields"][0]["value"]
        if "ç‰¹é¸" in v:
            pri = 1
        elif "æ³¨ç›®" in v:
            pri = 2
        elif "ãŠã™ã™ã‚" in v:
            pri = 3
        else:
            pri = 4

    mode_priority = 0 if e["fields"][1]["value"] == "æ—¢å­˜è²©å£²" else 1
    price = int(e["fields"][2]["value"].replace("å††", "").replace(",", ""))
    return (pri, mode_priority, price)


# ============================
# main
# ============================

def main(args):
    global seller_cache

    seller_cache = load_json(DATA_SELLER, default={})
    last = load_json(DATA_LAST, default={})

    now_ts = int(time.time())

    # å¤ã„ last ã‚’æ•´ç†ï¼ˆ30æ—¥ä»¥ä¸Šå‰ã‚’å‰Šé™¤ï¼‰
    last = {h: ts for h, ts in last.items() if isinstance(ts, int) and now_ts - ts < THIRTY_DAYS}

    try:
        # æœ6æ™‚ã¾ã¨ã‚é€šçŸ¥
        if is_morning() and not args.force_night:
            pending = load_json(DATA_PENDING_EXIST, default=[]) + load_json(
                DATA_PENDING_AUCTION, default=[]
            )
            if pending and not args.dry_run:
                send_discord(WEBHOOK_URL, "ğŸŒ… æ·±å¤œå¸¯ã¾ã¨ã‚é€šçŸ¥", pending[:10])

            clear_json(DATA_PENDING_EXIST)
            clear_json(DATA_PENDING_AUCTION)

        soup_exist = parse_html(fetch_html(URL_EXIST, retry=args.retry))
        soup_auction = parse_html(fetch_html(URL_AUCTION, retry=args.retry))

        items = []
        if soup_exist:
            items += parse_items(soup_exist, "exist")
        if soup_auction:
            items += parse_items(soup_auction, "auction")

        items.sort(key=lambda x: int(x["price"].replace("å††", "").replace(",", "")))

        embeds = []

        for item in items:
            key = normalize_url(item["url"])
            if not key:
                continue

            h = generate_item_hash(key)

            if h in last:
                continue

            price = int(item["price"].replace("å††", "").replace(",", ""))

            seller = fetch_seller_id(item["url"], no_cache=args.no_cache)

            if seller in SPECIAL_USERS:
                if len(embeds) < 10:
                    embeds.append(build_embed(item, seller))
                last[h] = now_ts
                continue

            if price >= 15000:
                last[h] = now_ts
                continue

            if not seller or seller in EXCLUDE_USERS:
                last[h] = now_ts
                continue

            if is_night_forced(args):
                path = DATA_PENDING_EXIST if item["mode"] == "exist" else DATA_PENDING_AUCTION
                append_json_list(path, item)
                last[h] = now_ts
                continue

            if len(embeds) < 10:
                embeds.append(build_embed(item, seller))

            last[h] = now_ts

        if embeds:
            embeds.sort(key=embed_priority)

            contains_special = any(e.get("seller") in SPECIAL_USERS for e in embeds)

            if contains_special:
                title = "@everyone\nğŸ’Œã¤ãªã å„ªå…ˆé€šçŸ¥"
            else:
                first_label = embeds[0]["fields"][0]["value"]
                if "ç‰¹é¸" in first_label:
                    title = "ğŸ”¥ã¤ãªã ç‰¹é¸é€šçŸ¥"
                elif "æ³¨ç›®" in first_label:
                    title = "â­ã¤ãªã æ³¨ç›®é€šçŸ¥"
                elif "ãŠã™ã™ã‚" in first_label:
                    title = "âœ¨ã¤ãªã ãŠã™ã™ã‚é€šçŸ¥"
                else:
                    title = "ğŸ“ã¤ãªã é€šå¸¸é€šçŸ¥"

            if args.dry_run:
                if not args.quiet:
                    print("=== DRY RUN ===")
                    print(title)
                    for e in embeds:
                        print(e)
            else:
                # last ã®ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆä»¶æ•°åˆ¶é™ï¼‰
                if len(last) > MAX_LAST:
                    sorted_items = sorted(last.items(), key=lambda x: x[1])
                    last = dict(sorted_items[-MAX_LAST:])

                ok = send_discord(WEBHOOK_URL, title, embeds)
                if ok:
                    save_json(DATA_LAST, last)
                else:
                    if not args.quiet:
                        print("é€ä¿¡å¤±æ•—ã®ãŸã‚ last_all.json ã¯æ›´æ–°ã—ã¾ã›ã‚“")

    finally:
        save_json(DATA_SELLER, seller_cache)


# ============================
# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ============================

if __name__ == "__main__":
    args = parse_args()
    main(args)
