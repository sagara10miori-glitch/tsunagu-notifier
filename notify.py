# ============================
# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# ============================

import os
import re
import time
import datetime
import requests

from utils.fetch import parse_html, validate_image_url
from utils.storage import load_json, save_json, append_json_list, clear_json
from utils.hashgen import generate_item_hash
from utils.shorturl import get_short_url
from utils.discord import send_discord


# ============================
# è¨­å®š
# ============================

WEBHOOK_URL = os.getenv("DISCORD_WEBHOOK_URL")

URL_EXIST = (
    "https://tsunagu.cloud/exist_products"
    "?sort=&exist_product_category_id=2"
    "&exist_product_category2_id=2"
    "&exist_product_category3_id="
    "&keyword=&max_sales_count_exist_items=1"
    "&is_selling=true&is_ai_content=0"
)

URL_AUCTION = (
    "https://tsunagu.cloud/auctions"
    "?sort=&exist_product_category_id=2"
    "&exist_product_category2_id=2"
    "&exist_product_category3_id="
    "&keyword=&is_disp_progress=1&is_ai_content=0"
)

DATA_LAST = "data/last_all.json"
DATA_SELLER = "data/seller_cache.json"
DATA_PENDING_EXIST = "data/pending_night_exist.json"
DATA_PENDING_AUCTION = "data/pending_night_auction.json"


# ============================
# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šèª­ã¿è¾¼ã¿
# ============================

def load_exclude_users(path):
    """é™¤å¤–ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {line.strip() for line in f if line.strip() and not line.startswith("#")}
    except FileNotFoundError:
        return set()


EXCLUDE_USERS = load_exclude_users("config/exclude_users.txt")


def load_special_users(path):
    """å„ªå…ˆé€šçŸ¥ãƒ¦ãƒ¼ã‚¶ãƒ¼ä¸€è¦§ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return {line.strip() for line in f if line.strip() and not line.startswith("#")}
    except FileNotFoundError:
        return set()


SPECIAL_USERS = load_special_users("config/special_users.txt")


# ============================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================

# GitHub Actions ã¯ UTC ã§å‹•ããŸã‚ã€JST ã«è£œæ­£ã—ãŸç¾åœ¨æ™‚åˆ»ã‚’è¿”ã™
def now():
    return datetime.datetime.utcnow() + datetime.timedelta(hours=9)


def is_night():
    """æ·±å¤œå¸¯ï¼ˆJST 2:00ã€œ5:59ï¼‰åˆ¤å®š"""
    h = now().hour
    return 2 <= h < 6


def is_morning():
    """æœ6:00ã¡ã‚‡ã†ã©ã®ã¾ã¨ã‚é€šçŸ¥åˆ¤å®š"""
    t = now()
    return t.hour == 6 and t.minute == 0


# ä¾¡æ ¼æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–ï¼ˆæ•°å­—æŠ½å‡º â†’ ã‚«ãƒ³ãƒä»˜ä¸ â†’ å††ï¼‰
def normalize_price(s):
    digits = "".join(c for c in s if c.isdigit())
    return f"{int(digits):,}å††" if digits else "0å††"


# URL æ­£è¦åŒ–ï¼ˆå•†å“IDéƒ¨åˆ†ã ã‘ã‚’æŠ½å‡ºï¼‰
_URL_RE = re.compile(r"(auctions|exist_products)/(\d+)")

def normalize_url(url):
    """å•†å“URLã‚’å®‰å®šã—ãŸã‚­ãƒ¼ã«å¤‰æ›"""
    m = _URL_RE.search(url)
    if m:
        return f"{m.group(1)}/{m.group(2)}"
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„ # ã‚’é™¤å»
    return url.split("?")[0].split("#")[0].rstrip("/")


# ============================
# Cloudflare ã«å¼·ã„ HTML fetch
# ============================

def fetch_html(url):
    """
    Cloudflare ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚’é¿ã‘ã¤ã¤ HTML ã‚’å–å¾—ã™ã‚‹ã€‚
    è»½é‡ãª retryï¼ˆæŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ï¼‰ã§å®‰å®šæ€§ã‚’ç¢ºä¿ã€‚
    """
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/121.0.0.0 Safari/537.36"
        ),
        "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    }

    proxy = os.getenv("PROXY_URL")
    proxies = {"http": proxy, "https": proxy} if proxy else None

    # 2å› â†’ 3å›ã«å¢—ã‚„ã—ã¦å®‰å®šæ€§UPï¼ˆä»•æ§˜ã¯å¤‰ã‚ã‚‰ãªã„ï¼‰
    for t in range(3):
        try:
            r = requests.get(url, headers=headers, proxies=proxies, timeout=6)
            r.raise_for_status()
            return r.text
        except Exception:
            # Cloudflare ã®ä¸€æ™‚ãƒ–ãƒ­ãƒƒã‚¯ã«å¼·ã„æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
            time.sleep(1.2 * (t + 1))

    return ""


# ============================
# seller_id æŠ½å‡ºï¼ˆé«˜é€Ÿãƒ»å®‰å®šï¼‰
# ============================

seller_cache = {}

def fetch_seller_id(url):
    """
    å•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ seller_id ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    - seller_cache ã«ã‚ˆã‚Šé«˜é€ŸåŒ–
    - Cloudflare ãƒ–ãƒ­ãƒƒã‚¯æ™‚ã‚‚ç©ºæ–‡å­—ã§å®‰å…¨ã«å‡¦ç†
    """
    if url in seller_cache:
        return seller_cache[url]

    html = fetch_html(url)
    if not html:
        seller_cache[url] = ""
        return ""

    soup = parse_html(html)
    if not soup:
        seller_cache[url] = ""
        return ""

    # /users/ ã¾ãŸã¯ /profile/ ã®ãƒªãƒ³ã‚¯ã‹ã‚‰ seller_id ã‚’æŠ½å‡º
    for pat in ["/users/", "/profile/"]:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if pat in href:
                m = re.search(pat + r"([^/?#]+)", href)
                if m:
                    seller_cache[url] = m.group(1)
                    return seller_cache[url]

    seller_cache[url] = ""
    return ""


# ============================
# seller_id æŠ½å‡ºï¼ˆé«˜é€Ÿãƒ»å®‰å®šï¼‰
# ============================

seller_cache = {}


def fetch_seller_id(url):
    if url in seller_cache:
        return seller_cache[url]

    html = fetch_html(url)
    if not html:
        seller_cache[url] = ""
        return ""

    soup = parse_html(html)
    if not soup:
        seller_cache[url] = ""
        return ""

    for pat in ["/users/", "/profile/"]:
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if pat in href:
                m = re.search(pat + r"([^/?#]+)", href)
                if m:
                    seller_cache[url] = m.group(1)
                    return seller_cache[url]

    seller_cache[url] = ""
    return ""


# ============================
# HTML è§£æï¼ˆèª¤æ¤œå‡ºã‚¼ãƒ­ãƒ»é«˜é€ŸåŒ–ï¼‰
# ============================

def parse_items(soup, mode):
    """
    å•†å“ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å•†å“æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    - ä¾¡æ ¼ã‚¿ã‚°ã®æ¤œå‡ºã‚’å¼·åŒ–ï¼ˆtext-danger ãŒç„¡ã„å ´åˆã® fallbackï¼‰
    - URL ã®è£œæ­£ï¼ˆ// â†’ https:ã€/ â†’ https://tsunagu.cloudï¼‰
    - ä»•æ§˜ã¯å®Œå…¨ã«ãã®ã¾ã¾
    """
    items = []

    for c in soup.find_all(class_="p-product"):
        # ã‚¿ã‚¤ãƒˆãƒ«
        t = c.find(class_="title")
        title = t.get_text(strip=True) if t else ""

        # ä¾¡æ ¼ï¼ˆtext-danger ãŒç„¡ã„å ´åˆã® fallbackï¼‰
        price_tag = c.find("p", class_=lambda x: x and "text-danger" in x)
        if not price_tag:
            # ä¾¡æ ¼ãŒåˆ¥ã‚¿ã‚°ã«å…¥ã£ã¦ã„ã‚‹ã‚±ãƒ¼ã‚¹ã«å¯¾å¿œ
            for tag in c.find_all(["p", "h2", "h3"]):
                txt = tag.get_text(strip=True)
                if ("å††" in txt or "Â¥" in txt) and any(ch.isdigit() for ch in txt):
                    price_tag = tag
                    break

        price = normalize_price(price_tag.get_text(strip=True) if price_tag else "")

        # å³æ±ºä¾¡æ ¼ï¼ˆã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³ã®ã¿ï¼‰
        buy_now = None
        h2 = c.find("h2")
        if h2 and ("å³æ±º" in h2.text):
            buy_now = normalize_price(h2.text)

        # å•†å“URL
        url = c.find("a")["href"]
        if url.startswith("//"):
            url = "https:" + url
        elif url.startswith("/"):
            url = "https://tsunagu.cloud" + url

        # ã‚µãƒ ãƒã‚¤ãƒ«
        img_tag = c.find("img")
        thumb = img_tag["src"] if img_tag else ""

        # å•†å“ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        items.append({
            "title": title,
            "price": price,
            "buy_now": buy_now,
            "thumb": thumb,
            "url": url,
            "mode": mode,
        })

    return items


# ============================
# embed ç”Ÿæˆï¼ˆçŸ­ããƒ»ç¾ã—ããƒ»sellerä¿æŒï¼‰
# ============================

def build_embed(item, seller):
    """
    Discord ã«é€ã‚‹ embed ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    - seller ã‚’ embed ã«ä¿æŒï¼ˆå†å–å¾—ä¸è¦ï¼‰
    - ä¾¡æ ¼ã«å¿œã˜ã¦è‰²åˆ†ã‘ï¼ˆä»•æ§˜ãã®ã¾ã¾ï¼‰
    - ã‚µãƒ ãƒã‚¤ãƒ«ã¯ validate_image_url ã§å®‰å…¨ã«å‡¦ç†
    """
    # ä¾¡æ ¼ã®æ•°å€¤åŒ–ï¼ˆè¤‡æ•°å›ä½¿ã†ã®ã§æœ€åˆã«å‡¦ç†ï¼‰
    p = int(item["price"].replace("å††", "").replace(",", ""))

    # ä¾¡æ ¼å¸¯ã«ã‚ˆã‚‹è‰²åˆ†ã‘ï¼ˆæ—¢å­˜ä»•æ§˜ã‚’ç¶­æŒï¼‰
    color = (
        0xE74C3C if p <= 5000 else
        0x3498DB if p <= 9999 else
        0x2ECC71
    )

    # URL ã¯çŸ­ç¸®ç‰ˆã‚’ä½¿ç”¨ï¼ˆæ—¢å­˜ä»•æ§˜ï¼‰
    short = get_short_url(item["url"])

    # embed ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
    fields = [
        {"name": "URL", "value": short, "inline": False},
        {
            "name": "è²©å£²å½¢å¼",
            "value": "æ—¢å­˜è²©å£²" if item["mode"] == "exist" else "ã‚ªãƒ¼ã‚¯ã‚·ãƒ§ãƒ³",
            "inline": True,
        },
        {"name": "ä¾¡æ ¼", "value": item["price"], "inline": True},
    ]

    # å³æ±ºä¾¡æ ¼ãŒã‚ã‚‹å ´åˆã®ã¿è¿½åŠ ï¼ˆæ—¢å­˜ä»•æ§˜ï¼‰
    if item["buy_now"]:
        fields.append({
            "name": "å³æ±ºä¾¡æ ¼",
            "value": item["buy_now"],
            "inline": True
        })

    # embed æœ¬ä½“
    embed = {
        "title": item["title"][:256],  # Discord ã®åˆ¶é™ã«åˆã‚ã›ã‚‹
        "url": short,
        "color": color,
        "fields": fields,
        "seller": seller,  # â† é‡è¦ï¼šseller ã‚’ embed ã«ä¿æŒ
    }

    # ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
    img = validate_image_url(item["thumb"])
    if img:
        embed["image"] = {"url": img}

    return embed


# ============================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================

def main():
    global seller_cache

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
    seller_cache = load_json(DATA_SELLER, default={})
    last = load_json(DATA_LAST, default={})

    try:
        # ============================
        # æœ6:00 â†’ æ·±å¤œå¸¯ã¾ã¨ã‚é€šçŸ¥
        # ============================
        if is_morning():
            pending = (
                load_json(DATA_PENDING_EXIST, []) +
                load_json(DATA_PENDING_AUCTION, [])
            )
            if pending:
                send_discord(WEBHOOK_URL, "ğŸŒ… æ·±å¤œå¸¯ã¾ã¨ã‚é€šçŸ¥", pending[:10])

            clear_json(DATA_PENDING_EXIST)
            clear_json(DATA_PENDING_AUCTION)

        # ============================
        # å•†å“ä¸€è¦§å–å¾—
        # ============================
        soup_exist = parse_html(fetch_html(URL_EXIST))
        soup_auction = parse_html(fetch_html(URL_AUCTION))

        items = []
        if soup_exist:
            items += parse_items(soup_exist, "exist")
        if soup_auction:
            items += parse_items(soup_auction, "auction")

        # ä¾¡æ ¼ã®å®‰ã„é †ã«ä¸¦ã¹ã‚‹ï¼ˆæ—¢å­˜ä»•æ§˜ï¼‰
        items.sort(key=lambda x: int(x["price"].replace("å††", "").replace(",", "")))

        embeds = []

        # ============================
        # å•†å“ã”ã¨ã®å‡¦ç†
        # ============================
        for item in items:
            key = normalize_url(item["url"])
            h = generate_item_hash(key)

            # ã™ã§ã«é€šçŸ¥æ¸ˆã¿ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
            if h in last:
                continue

            # ä¾¡æ ¼ã®æ•°å€¤åŒ–ï¼ˆè¤‡æ•°å›ä½¿ã†ã®ã§æœ€åˆã«ï¼‰
            price = int(item["price"].replace("å††", "").replace(",", ""))

            # seller_id ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚ã‚Šï¼‰
            seller = fetch_seller_id(item["url"])

            # ============================
            # special_users â†’ æœ€å„ªå…ˆã§é€šçŸ¥
            # ============================
            if seller in SPECIAL_USERS:
                if len(embeds) < 10:
                    embeds.append(build_embed(item, seller))
                last[h] = True
                continue

            # ============================
            # é€šå¸¸ã®ä¾¡æ ¼ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆ15000å††ä»¥ä¸Šã¯é€šçŸ¥ã—ãªã„ï¼‰
            # ============================
            if price >= 15000:
                last[h] = True
                continue

            # ============================
            # é™¤å¤–ãƒ¦ãƒ¼ã‚¶ãƒ¼
            # ============================
            if not seller or seller in EXCLUDE_USERS:
                last[h] = True
                continue

            # ============================
            # æ·±å¤œå¸¯ â†’ pending ã«ä¿å­˜
            # ============================
            if is_night():
                path = DATA_PENDING_EXIST if item["mode"] == "exist" else DATA_PENDING_AUCTION
                append_json_list(path, item)
                last[h] = True
                continue

            # ============================
            # é€šå¸¸é€šçŸ¥
            # ============================
            if len(embeds) < 10:
                embeds.append(build_embed(item, seller))

            last[h] = True

        # ============================
        # é€šçŸ¥é€ä¿¡
        # ============================
        if embeds:
            # special_users ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹åˆ¤å®šï¼ˆembed ã« seller ã‚’ä¿æŒã—ã¦ã„ã‚‹ï¼‰
            contains_special = any(
                embed.get("seller") in SPECIAL_USERS
                for embed in embeds
            )

            if contains_special:
                # @everyone ã¯å„ªå…ˆé€šçŸ¥ã®ã¨ãã ã‘
                title = "@everyone\nğŸ’Œã¤ãªãã€€å„ªå…ˆé€šçŸ¥"
            else:
                first_price = int(
                    embeds[0]["fields"][2]["value"].replace("å††", "").replace(",", "")
                )
                title = (
                    "ğŸ“¢ã¤ãªãã€€æ–°ç€é€šçŸ¥" if first_price <= 5000 else
                    "ğŸ””ã¤ãªãã€€æ–°ç€é€šçŸ¥" if first_price <= 9999 else
                    "ğŸ“ã¤ãªãã€€æ–°ç€é€šçŸ¥"
                )

            send_discord(WEBHOOK_URL, title, embeds)

    finally:
        # ä¿å­˜é †åºã‚’ seller_cache â†’ last ã«ã—ã¦å®‰å…¨æ€§UP
        save_json(DATA_SELLER, seller_cache)
        save_json(DATA_LAST, last)


# ============================
# ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
# ============================

if __name__ == "__main__":
    main()
